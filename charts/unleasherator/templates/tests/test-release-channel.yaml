---
# ServiceAccount for the release-channel test job
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-channel-test-sa
  labels:
    app.kubernetes.io/name: release-channel-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-20"
    "helm.sh/hook-delete-policy": before-hook-creation
---
# ClusterRole for reading Unleash and ReleaseChannel resources
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-channel-test-role
  labels:
    app.kubernetes.io/name: release-channel-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-20"
    "helm.sh/hook-delete-policy": before-hook-creation
rules:
- apiGroups: ["unleash.nais.io"]
  resources: ["unleashes", "releasechannels", "apitokens"]
  verbs: ["get", "list", "watch", "patch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods", "secrets"]
  verbs: ["get", "list", "watch"]
---
# ClusterRoleBinding to grant permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-channel-test-binding
  labels:
    app.kubernetes.io/name: release-channel-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-20"
    "helm.sh/hook-delete-policy": before-hook-creation
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-channel-test-role
subjects:
- kind: ServiceAccount
  name: release-channel-test-sa
  namespace: {{ .Release.Namespace }}
---
# PostgreSQL database for the release channel test
apiVersion: v1
kind: Secret
metadata:
  name: postgres-rc-test
  labels:
    app.kubernetes.io/name: postgresql-rc-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-10"
    "helm.sh/hook-delete-policy": before-hook-creation
type: Opaque
data:
  postgres-password: cG9zdGdyZXNwYXNzd29yZA== # postgrespassword
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-rc-init-scripts
  labels:
    app.kubernetes.io/name: postgresql-rc-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-10"
    "helm.sh/hook-delete-policy": before-hook-creation
data:
  init-databases.sh: |
    #!/bin/bash
    set -e
    # Create separate databases for each Unleash instance to avoid conflicts
    psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" --dbname "$POSTGRES_DB" <<-EOSQL
        CREATE DATABASE unleash_prod;
        CREATE DATABASE unleash_staging;
        CREATE DATABASE unleash_custom;
        GRANT ALL PRIVILEGES ON DATABASE unleash_prod TO postgres;
        GRANT ALL PRIVILEGES ON DATABASE unleash_staging TO postgres;
        GRANT ALL PRIVILEGES ON DATABASE unleash_custom TO postgres;
    EOSQL
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-rc-test
  labels:
    app.kubernetes.io/name: postgresql-rc-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-10"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql-rc-test
  template:
    metadata:
      labels:
        app.kubernetes.io/name: postgresql-rc-test
    spec:
      containers:
      - name: postgresql
        image: postgres:15.3
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_DB
          value: postgres
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-rc-test
              key: postgres-password
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: init-scripts
          mountPath: /docker-entrypoint-initdb.d
      volumes:
      - name: postgres-data
        emptyDir: {}
      - name: init-scripts
        configMap:
          name: postgres-rc-init-scripts
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-rc-test
  labels:
    app.kubernetes.io/name: postgresql-rc-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-10"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  ports:
  - port: 5432
    targetPort: 5432
  selector:
    app.kubernetes.io/name: postgresql-rc-test
---
# ReleaseChannel with canary deployment strategy for testing
apiVersion: unleash.nais.io/v1
kind: ReleaseChannel
metadata:
  name: release-channel-test
  labels:
    app.kubernetes.io/name: release-channel-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  image: "unleashorg/unleash-server:6.10.0"
  strategy:
    canary:
      enabled: true
      podSelector:
        matchLabels:
          deployment: "staging"
    maxParallel: 1
    batchInterval: "10s"
    maxUpgradeTime: "5m"
  healthChecks:
    enabled: true
    initialDelay: "10s"
    timeout: "2m"
  rollback:
    enabled: false
    onFailure: true
---
# First Unleash instance managed by the release channel
apiVersion: unleash.nais.io/v1
kind: Unleash
metadata:
  name: unleash-rc-prod
  labels:
    app.kubernetes.io/name: unleash-rc-prod
    app.kubernetes.io/part-of: unleasherator
    deployment: "production"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  size: 1
  database:
    secretName: postgres-rc-test
    secretPassKey: postgres-password
    host: postgres-rc-test
    databaseName: unleash_prod
    port: "5432"
    user: postgres
    ssl: "false"
  releaseChannel:
    name: release-channel-test
  extraEnvVars:
    - name: ENABLE_OAS
      value: "true"
  networkPolicy:
    enabled: true
    allowDNS: true
    extraEgressRules:
      - to:
          - podSelector:
              matchLabels:
                app.kubernetes.io/name: postgresql-rc-test
        ports:
          - protocol: TCP
            port: 5432
---
# Second Unleash instance managed by the same release channel
apiVersion: unleash.nais.io/v1
kind: Unleash
metadata:
  name: unleash-rc-staging
  labels:
    app.kubernetes.io/name: unleash-rc-staging
    app.kubernetes.io/part-of: unleasherator
    deployment: "staging"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  size: 1
  database:
    secretName: postgres-rc-test
    secretPassKey: postgres-password
    host: postgres-rc-test
    databaseName: unleash_staging
    port: "5432"
    user: postgres
    ssl: "false"
  releaseChannel:
    name: release-channel-test
  extraEnvVars:
    - name: ENABLE_OAS
      value: "true"
  networkPolicy:
    enabled: true
    allowDNS: true
    extraEgressRules:
      - to:
          - podSelector:
              matchLabels:
                app.kubernetes.io/name: postgresql-rc-test
        ports:
          - protocol: TCP
            port: 5432
---
# Third Unleash instance with customImage (should be ignored by ReleaseChannel)
apiVersion: unleash.nais.io/v1
kind: Unleash
metadata:
  name: unleash-rc-custom
  labels:
    app.kubernetes.io/name: unleash-rc-custom
    app.kubernetes.io/part-of: unleasherator
    deployment: "custom"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  size: 1
  customImage: "unleashorg/unleash-server:5.12.0"
  database:
    secretName: postgres-rc-test
    secretPassKey: postgres-password
    host: postgres-rc-test
    databaseName: unleash_custom
    port: "5432"
    user: postgres
    ssl: "false"
  releaseChannel:
    name: release-channel-test
  extraEnvVars:
    - name: ENABLE_OAS
      value: "true"
  networkPolicy:
    enabled: true
    allowDNS: true
    extraEgressRules:
      - to:
          - podSelector:
              matchLabels:
                app.kubernetes.io/name: postgresql-rc-test
        ports:
          - protocol: TCP
            port: 5432
---
# API Token for the production instance
apiVersion: unleash.nais.io/v1
kind: ApiToken
metadata:
  name: unleash-rc-prod-token
  labels:
    app.kubernetes.io/name: unleash-rc-prod
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "0"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  unleashInstance:
    apiVersion: unleash.nais.io/v1
    kind: Unleash
    name: unleash-rc-prod
  secretName: unleash-rc-prod-token
  type: CLIENT
  environment: development
  projects:
    - default
---
# API Token for the staging instance
apiVersion: unleash.nais.io/v1
kind: ApiToken
metadata:
  name: unleash-rc-staging-token
  labels:
    app.kubernetes.io/name: unleash-rc-staging
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "0"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  unleashInstance:
    apiVersion: unleash.nais.io/v1
    kind: Unleash
    name: unleash-rc-staging
  secretName: unleash-rc-staging-token
  type: CLIENT
  environment: development
  projects:
    - default
---
# Test job to verify ReleaseChannel functionality
apiVersion: batch/v1
kind: Job
metadata:
  name: unleasherator-rc-test-probe
  labels:
    app.kubernetes.io/name: release-channel-test
    app.kubernetes.io/part-of: unleasherator
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-weight": "5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  backoffLimit: 3
  activeDeadlineSeconds: {{ ((.Values.test).jobDeadlineSeconds) | default 420 }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: release-channel-test
        app.kubernetes.io/part-of: unleasherator
    spec:
      serviceAccountName: release-channel-test-sa
      initContainers:
      - name: wait-for-postgres
        image: bitnami/kubectl:latest
        command:
        - sh
        - -c
        - |
          echo "Waiting for PostgreSQL to be ready..."
          MAX_ATTEMPTS={{ ((.Values.test).waitIterations) | default 60 }}
          INTERVAL={{ ((.Values.test).intervalSeconds) | default 5 }}
          for i in $(seq 1 $MAX_ATTEMPTS); do
            PG_READY=$(kubectl get deployment postgres-rc-test -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
            if [ "$PG_READY" = "1" ]; then
              echo "✓ PostgreSQL is ready"
              exit 0
            fi
            echo "  Attempt $i/$MAX_ATTEMPTS: PostgreSQL not ready yet..."
            sleep $INTERVAL
          done
          echo "✗ PostgreSQL did not become ready"
          exit 1
      - name: wait-for-unleash-instances
        image: bitnami/kubectl:latest
        command:
        - sh
        - -c
        - |
          echo "Waiting for Unleash instances to be ready..."
          MAX_ATTEMPTS={{ ((.Values.test).waitIterations) | default 60 }}
          INTERVAL={{ ((.Values.test).intervalSeconds) | default 5 }}
          for instance in unleash-rc-prod unleash-rc-staging unleash-rc-custom; do
            echo "Checking $instance..."
            for i in $(seq 1 $MAX_ATTEMPTS); do
              CONNECTED=$(kubectl get unleash $instance -o jsonpath='{.status.connected}' 2>/dev/null || echo "false")
              if [ "$CONNECTED" = "true" ]; then
                echo "✓ $instance is connected!"
                break
              fi
              if [ $i -eq $MAX_ATTEMPTS ]; then
                echo "✗ $instance did not become ready within timeout"
                kubectl get unleash $instance -o yaml || true
                exit 1
              fi
              if [ $((i % 10)) -eq 0 ]; then
                echo "  Attempt $i/$MAX_ATTEMPTS: $instance connected=$CONNECTED, waiting..."
              fi
              sleep $INTERVAL
            done
          done
          echo "✓ All Unleash instances are ready"
      containers:
      - name: release-channel-test
        image: bitnami/kubectl:latest
        command:
        - sh
        - -c
        - |
          set -e
          echo "=== ReleaseChannel E2E Test Suite ==="
          echo "Testing comprehensive ReleaseChannel features including canary deployment"
          echo ""

          # Verify ReleaseChannel exists and is configured
          echo "1. Verifying ReleaseChannel configuration..."
          RC_IMAGE=$(kubectl get releasechannel release-channel-test -o jsonpath='{.spec.image}')
          CANARY_ENABLED=$(kubectl get releasechannel release-channel-test -o jsonpath='{.spec.strategy.canary.enabled}')
          MAX_PARALLEL=$(kubectl get releasechannel release-channel-test -o jsonpath='{.spec.strategy.maxParallel}')

          echo "  Image: $RC_IMAGE (expected: unleashorg/unleash-server:6.10.0)"
          echo "  Canary enabled: $CANARY_ENABLED (expected: true)"
          echo "  MaxParallel: $MAX_PARALLEL (expected: 1)"

          if [ "$RC_IMAGE" != "unleashorg/unleash-server:6.10.0" ]; then
            echo "✗ ReleaseChannel image mismatch"
            exit 1
          fi
          if [ "$CANARY_ENABLED" != "true" ]; then
            echo "✗ Canary should be enabled"
            exit 1
          fi
          echo "✓ ReleaseChannel spec correctly configured"

          # Verify CustomImage exclusion
          echo ""
          echo "2. Verifying CustomImage exclusion..."
          CUSTOM_RESOLVED=$(kubectl get unleash unleash-rc-custom -o jsonpath='{.status.resolvedReleaseChannelImage}' 2>/dev/null || echo "")
          CUSTOM_IMAGE=$(kubectl get deployment unleash-rc-custom -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "")

          if [ -z "$CUSTOM_RESOLVED" ] && echo "$CUSTOM_IMAGE" | grep -q "5.12.0"; then
            echo "✓ Instance with customImage correctly ignored by ReleaseChannel"
            echo "  Deployment uses custom image: $CUSTOM_IMAGE"
          else
            echo "✗ Instance with customImage should not be managed by ReleaseChannel"
            echo "  resolvedReleaseChannelImage: $CUSTOM_RESOLVED (should be empty)"
            echo "  deployment image: $CUSTOM_IMAGE (should contain 5.12.0)"
            exit 1
          fi

          # Verify ReleaseChannel status
          echo ""
          echo "3. Verifying ReleaseChannel status..."
          RC_PHASE=$(kubectl get releasechannel release-channel-test -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
          RC_INSTANCES=$(kubectl get releasechannel release-channel-test -o jsonpath='{.status.instances}' 2>/dev/null || echo "0")
          RC_CANARY=$(kubectl get releasechannel release-channel-test -o jsonpath='{.status.canaryInstances}' 2>/dev/null || echo "0")

          echo "  Phase: $RC_PHASE"
          echo "  Total Instances: $RC_INSTANCES (expected: 2, not 3 due to customImage)"
          echo "  Canary Instances: $RC_CANARY (expected: 1 - staging)"

          if [ "$RC_INSTANCES" != "2" ]; then
            echo "✗ Expected 2 instances (excluding customImage instance), got: $RC_INSTANCES"
            kubectl get releasechannel release-channel-test -o yaml
            exit 1
          fi

          if [ "$RC_CANARY" != "1" ]; then
            echo "✗ Expected 1 canary instance (staging), got: $RC_CANARY"
            exit 1
          fi
          echo "✓ ReleaseChannel correctly manages 2 instances with 1 canary"

          # Verify managed instances use ReleaseChannel image
          echo ""
          echo "4. Verifying managed instances use ReleaseChannel image..."
          for instance in unleash-rc-prod unleash-rc-staging; do
            RESOLVED_IMAGE=$(kubectl get unleash $instance -o jsonpath='{.status.resolvedReleaseChannelImage}' 2>/dev/null || echo "")
            RC_NAME=$(kubectl get unleash $instance -o jsonpath='{.status.releaseChannelName}' 2>/dev/null || echo "")

            if [ "$RESOLVED_IMAGE" = "unleashorg/unleash-server:6.10.0" ] && [ "$RC_NAME" = "release-channel-test" ]; then
              echo "  ✓ $instance: image=$RESOLVED_IMAGE, channel=$RC_NAME"
            else
              echo "  ✗ $instance image mismatch"
              echo "    Expected image: unleashorg/unleash-server:6.10.0, Got: $RESOLVED_IMAGE"
              echo "    Expected channel: release-channel-test, Got: $RC_NAME"
              exit 1
            fi
          done

          # Test canary deployment: Update to newer version
          echo ""
          echo "5. Testing canary deployment scenario..."
          echo "  Updating ReleaseChannel from 6.10.0 to 6.10.1..."
          kubectl patch releasechannel release-channel-test --type='merge' -p='{"spec":{"image":"unleashorg/unleash-server:6.10.1"}}'

          # Track canary phase progression
          echo ""
          echo "6. Monitoring canary phase progression..."
          MAX_ATTEMPTS={{ ((.Values.test).waitIterations) | default 60 }}
          INTERVAL={{ ((.Values.test).connectionIntervalSeconds) | default 3 }}
          for i in $(seq 1 $MAX_ATTEMPTS); do
            PHASE=$(kubectl get releasechannel release-channel-test -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
            STAGING_IMAGE=$(kubectl get unleash unleash-rc-staging -o jsonpath='{.status.resolvedReleaseChannelImage}' 2>/dev/null || echo "")
            PROD_IMAGE=$(kubectl get unleash unleash-rc-prod -o jsonpath='{.status.resolvedReleaseChannelImage}' 2>/dev/null || echo "")

            # Check if canary (staging) updates before production
            if [ "$STAGING_IMAGE" = "unleashorg/unleash-server:6.10.1" ] && [ "$PROD_IMAGE" = "unleashorg/unleash-server:6.10.0" ]; then
              echo "  ✓ Canary (staging) updated to 6.10.1 before production"
            fi

            # Check completion
            if [ "$STAGING_IMAGE" = "unleashorg/unleash-server:6.10.1" ] && [ "$PROD_IMAGE" = "unleashorg/unleash-server:6.10.1" ]; then
              echo "  ✓ Both instances successfully updated to 6.10.1"
              break
            fi

            if [ $((i % 10)) -eq 0 ]; then
              echo "    Progress check $i/$MAX_ATTEMPTS: phase=$PHASE, staging=$STAGING_IMAGE, prod=$PROD_IMAGE"
            fi

            sleep $INTERVAL
          done

          # Final status verification
          echo ""
          echo "7. Final status verification..."
          FINAL_STAGING=$(kubectl get unleash unleash-rc-staging -o jsonpath='{.status.resolvedReleaseChannelImage}')
          FINAL_PROD=$(kubectl get unleash unleash-rc-prod -o jsonpath='{.status.resolvedReleaseChannelImage}')
          FINAL_PHASE=$(kubectl get releasechannel release-channel-test -o jsonpath='{.status.phase}')

          echo "  Final Phase: $FINAL_PHASE"
          echo "  Staging instance: $FINAL_STAGING"
          echo "  Production instance: $FINAL_PROD"

          # We accept partial completion in CI environment
          SUCCESS_COUNT=0
          [ "$FINAL_STAGING" = "unleashorg/unleash-server:6.10.1" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          [ "$FINAL_PROD" = "unleashorg/unleash-server:6.10.1" ] && SUCCESS_COUNT=$((SUCCESS_COUNT + 1))

          if [ $SUCCESS_COUNT -eq 2 ]; then
            echo "✓ All managed instances successfully upgraded"
          elif [ $SUCCESS_COUNT -eq 1 ]; then
            echo "⚠ Partial upgrade completed (1/2 instances)"
            echo "  This is acceptable in CI environment with timing constraints"
          else
            echo "✗ Upgrade did not complete as expected"
            kubectl get releasechannel release-channel-test -o yaml
            exit 1
          fi

          echo ""
          echo "=== ReleaseChannel E2E Tests Completed Successfully ==="
          echo "✓ Canary deployment strategy configured"
          echo "✓ CustomImage exclusion validated"
          echo "✓ Status fields tracking (instances, canaryInstances)"
          echo "✓ Image upgrade rollout tested"
      restartPolicy: Never
