controllerManager:
  kubeRbacProxy:
    args:
    - --secure-listen-address=0.0.0.0:8443
    - --upstream=http://127.0.0.1:8080/
    - --logtostderr=true
    - --v=0
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    image:
      repository: gcr.io/kubebuilder/kube-rbac-proxy
      tag: v0.13.0
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 5m
        memory: 64Mi
  manager:
    args:
    - --config=controller_manager_config.yaml
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    env:
      apiTokenNameSuffix: unleasherator
      federationClusterName: ""
      federationPubsubGcpProjectId: ""
      federationPubsubMode: ""
      federationPubsubSubscription: ""
      federationPubsubTopic: ""
      googleApplicationCredentials: ""
      httpsProxy: ""
      noProxy: ""
    image:
      repository: ghcr.io/nais/unleasherator
      tag: main
    imagePullPolicy: Always
    resources:
      limits:
        memory: 512Mi
      requests:
        cpu: 50m
        memory: 256Mi
  replicas: 1
  serviceAccount:
    annotations: {}
controllerManagerGcp:
  serviceAccountCredentialsJson: ""
kubernetesClusterDomain: cluster.local
managerConfig:
  controllerManagerConfigYaml: |-
    apiVersion: controller-runtime.sigs.k8s.io/v1alpha1
    kind: ControllerManagerConfig
    metadata:
      labels:
        app.kubernetes.io/name: controllermanagerconfig
        app.kubernetes.io/instance: controller-manager-configuration
        app.kubernetes.io/component: manager
        app.kubernetes.io/created-by: unleasherator
        app.kubernetes.io/part-of: unleasherator
        app.kubernetes.io/managed-by: kustomize
    health:
      healthProbeBindAddress: :8081
    metrics:
      bindAddress: 127.0.0.1:8080
    webhook:
      port: 9443
    leaderElection:
      leaderElect: true
      resourceName: 509984d3.nais.io
    # leaderElectionReleaseOnCancel defines if the leader should step down volume
    # when the Manager ends. This requires the binary to immediately end when the
    # Manager is stopped, otherwise, this setting is unsafe. Setting this significantly
    # speeds up voluntary leader transitions as the new leader don't have to wait
    # LeaseDuration time first.
    # In the default scaffold provided, the program ends immediately after
    # the manager stops, so would be fine to enable this option. However,
    # if you are doing or is intended to do any operation such as perform cleanups
    # after the manager stops then its usage might be unsafe.
    # leaderElectionReleaseOnCancel: true
metricsService:
  ports:
  - name: https
    port: 8443
    protocol: TCP
    targetPort: https
  type: ClusterIP
